<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>1 基本回归分析 | 06-regression.utf8.md</title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="1 基本回归分析 | 06-regression.utf8.md" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 基本回归分析 | 06-regression.utf8.md" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89938436-1', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="part"><span><b>I Data Modeling via moderndive</b></span></li>
<li class="chapter" data-level="1" data-path=""><a href="#regression"><i class="fa fa-check"></i><b>1</b> 基本回归分析</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#-packages"><i class="fa fa-check"></i>所需的 packages</a></li>
<li class="chapter" data-level="" data-path=""><a href="#datacamp"><i class="fa fa-check"></i>DataCamp</a></li>
<li class="chapter" data-level="1.1" data-path=""><a href="#model1"><i class="fa fa-check"></i><b>1.1</b> 单个数值型解释变量</a><ul>
<li class="chapter" data-level="1.1.1" data-path=""><a href="#model1EDA"><i class="fa fa-check"></i><b>1.1.1</b> 数据探索分析</a></li>
<li class="chapter" data-level="1.1.2" data-path=""><a href="#model1table"><i class="fa fa-check"></i><b>1.1.2</b> 一元线性回归</a></li>
<li class="chapter" data-level="1.1.3" data-path=""><a href="#model1points"><i class="fa fa-check"></i><b>1.1.3</b> 观察值,拟合值,残差</a></li>
<li class="chapter" data-level="1.1.4" data-path=""><a href="#model1residuals"><i class="fa fa-check"></i><b>1.1.4</b> 残差分析</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#model2"><i class="fa fa-check"></i><b>1.2</b> 单个类别型解释变量</a><ul>
<li class="chapter" data-level="1.2.1" data-path=""><a href="#model2EDA"><i class="fa fa-check"></i><b>1.2.1</b> 数据探索分析</a></li>
<li class="chapter" data-level="1.2.2" data-path=""><a href="#model2table"><i class="fa fa-check"></i><b>1.2.2</b> 线性回归</a></li>
<li class="chapter" data-level="1.2.3" data-path=""><a href="#model2points"><i class="fa fa-check"></i><b>1.2.3</b> 观察值,拟合值,残差</a></li>
<li class="chapter" data-level="1.2.4" data-path=""><a href="#model2residuals"><i class="fa fa-check"></i><b>1.2.4</b> 残差分析</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#section-1.3"><i class="fa fa-check"></i><b>1.3</b> 其他内容</a><ul>
<li class="chapter" data-level="1.3.1" data-path=""><a href="#correlationcoefficient"><i class="fa fa-check"></i><b>1.3.1</b> 相关系数</a></li>
<li class="chapter" data-level="1.3.2" data-path=""><a href="#correlation-is-not-causation"><i class="fa fa-check"></i><b>1.3.2</b> 相关关系不一定是因果关系</a></li>
<li class="chapter" data-level="1.3.3" data-path=""><a href="#leastsquares"><i class="fa fa-check"></i><b>1.3.3</b> 最佳拟合线</a></li>
<li class="chapter" data-level="1.3.4" data-path=""><a href="#underthehood"><i class="fa fa-check"></i><b>1.3.4</b> <code>get_regression_x()</code> 功能</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#section-1.4"><i class="fa fa-check"></i><b>1.4</b> 结论</a><ul>
<li class="chapter" data-level="1.4.1" data-path=""><a href="#r"><i class="fa fa-check"></i><b>1.4.1</b> R代码</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<html>
<img src='https://moderndive.com/images/logos/wide_format.png' alt="ModernDive">
</html>
<!--bookdown:title:end-->
<!--bookdown:title:start-->
<div id="regression" class="section level1">
<h1><span class="header-section-number">1</span> 基本回归分析</h1>
<p>我们已经在之前的第3章学习了是数据图示化，第4章中了解了 “tidy” 数据结构的，第5章中学习了数据整理技能。现在我们开始进入数据建模的内容。数据建模的基础</p>
<p>Now that we are equipped with data visualization skills from Chapter <a href="#viz"><strong>??</strong></a>, an understanding of the “tidy” data format from Chapter <a href="#tidy"><strong>??</strong></a>, and data wrangling skills from Chapter <a href="#wrangling"><strong>??</strong></a>, we now proceed with data modeling. The fundamental premise of data modeling is <em>to make explicit the relationship</em> between:</p>
<ul>
<li>an outcome variable <span class="math inline">\(y\)</span>, also called a dependent variable and</li>
<li>an explanatory/predictor variable <span class="math inline">\(x\)</span>, also called an independent variable or covariate.</li>
</ul>
<p>Another way to state this is using mathematical terminology: we will model the outcome variable <span class="math inline">\(y\)</span> <em>as a function</em> of the explanatory/predictor variable <span class="math inline">\(x\)</span>. Why do we have two different labels, explanatory and predictor, for the variable <span class="math inline">\(x\)</span>? That’s because roughly speaking data modeling can be used for two purposes:</p>
<ol style="list-style-type: decimal">
<li><strong>Modeling for prediction</strong>: You want to predict an outcome variable <span class="math inline">\(y\)</span> based on the information contained in a set of predictor variables. You don’t care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about <span class="math inline">\(y\)</span>, you’re fine. For example, if we know many individuals’ risk factors for lung cancer, such as smoking habits and age, can we predict whether or not they will develop lung cancer? Here we wouldn’t care so much about distinguishing the degree to which the different risk factors contribute to lung cancer, but instead only on whether or not they could be put together to make reliable predictions.</li>
<li><strong>Modeling for explanation</strong>: You want to explicitly describe the relationship between an outcome variable <span class="math inline">\(y\)</span> and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these. Continuing our example from above, we would now be interested in describing the individual effects of the different risk factors and quantifying the magnitude of these effects. One reason could be to design an intervention to reduce lung cancer cases in a population, such as targeting smokers of a specific age group with an advertisement for smoking cessation programs. In this book, we’ll focus more on this latter purpose.</li>
</ol>
<p>Data modeling is used in a wide variety of fields, including statistical inference, causal inference, artificial intelligence, and machine learning. There are many techniques for data modeling, such as tree-based models, neural networks and deep learning, and supervised learning. In this chapter, we’ll focus on one particular technique: <em>linear regression</em>, one of the most commonly-used and easy-to-understand approaches to modeling. Recall our discussion in Subsection <a href="#exploredataframes"><strong>??</strong></a> on numerical and categorical variables. Linear regression involves:</p>
<ul>
<li>an outcome variable <span class="math inline">\(y\)</span> that is <em>numerical</em> and</li>
<li>explanatory variables <span class="math inline">\(\vec{x}\)</span> that are either <em>numerical</em> or <em>categorical</em>.</li>
</ul>
<p>With linear regression there is always only one numerical outcome variable <span class="math inline">\(y\)</span> but we have choices on both the number and the type of explanatory variables <span class="math inline">\(\vec{x}\)</span> to use. We’re going to cover the following regression scenarios:</p>
<ul>
<li>In this current chapter on basic regression, we’ll always have only one explanatory variable.
<ul>
<li>In Section <a href="#model1">1.1</a>, this explanatory variable will be a single numerical explanatory variable <span class="math inline">\(x\)</span>. This scenario is known as <em>simple linear regression</em>.</li>
<li>In Section <a href="#model2">1.2</a>, this explanatory variable will be a categorical explanatory variable <span class="math inline">\(x\)</span>.</li>
</ul></li>
<li>In the next chapter, Chapter <a href="#multiple-regression"><strong>??</strong></a> on <em>multiple regression</em>, we’ll have more than one explanatory variable:
<ul>
<li>We’ll focus on two numerical explanatory variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> in Section <a href="#model3"><strong>??</strong></a>. This can be denoted as <span class="math inline">\(\vec{x}\)</span> as well since we have more than one explanatory variable.</li>
<li>We’ll use one numerical and one categorical explanatory variable in Section <a href="#model3"><strong>??</strong></a>. We’ll also introduce <em>interaction models</em> here; there, the effect of one explanatory variable depends on the value of another.</li>
</ul></li>
</ul>
<p>We’ll study all four of these regression scenarios using real data, all easily accessible via R packages! <!--We will also discuss the concept of *correlation* and how it is frequently incorrectly used to imply *causation*.--></p>
<div id="-packages" class="section level3 unnumbered">
<h3>所需的 packages</h3>
<p>In this chapter we introduce a new package, <code>moderndive</code>, that is an accompaniment package to this ModernDive book. It includes useful functions for linear regression and other functions as well as data used later in the book. Let’s now load all the packages needed for this chapter. If needed, read Section <a href="#packages"><strong>??</strong></a> for information on how to install and load R packages.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(moderndive)
<span class="kw">library</span>(gapminder)
<span class="kw">library</span>(skimr)</code></pre>
</div>
<div id="datacamp" class="section level3 unnumbered">
<h3>DataCamp</h3>
<p>The introductory basic regression analysis below was the inspiration for a large part of ModernDive co-author <a href="https://twitter.com/rudeboybert">Albert Y. Kim’s</a> DataCamp course “Modeling with Data in the Tidyverse.” If you’re interested in complementing your learning below in an interactive online environment, click on the image below to access the course. The relevant chapters are Chapter 1 “Introduction to Modeling” and Chapter 2 “Modeling with Basic Regression”.</p>
<center>
<a target="_blank" class="page-link" href="https://www.datacamp.com/courses/modeling-with-data-in-the-tidyverse"><img src="images/datacamp_intro_to_modeling.png" alt="Drawing" style="height: 150px;"/></a>
</center>
</div>
<div id="model1" class="section level2">
<h2><span class="header-section-number">1.1</span> 单个数值型解释变量</h2>
<p>Why do some professors and instructors at universities and colleges get high teaching evaluations from students while others don’t? What factors can explain these differences? Are there biases? These are questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which professors and instructors should get promotions. Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer this question: what factors can explain differences in instructor’s teaching evaluation scores? To this end, they collected information on <span class="math inline">\(n = 463\)</span> instructors. A full description of the study can be found at <a href="https://www.openintro.org/stat/data/?data=evals">openintro.org</a>.</p>
<p>We’ll keep things simple for now and try to explain differences in instructor evaluation scores as a function of one numerical variable: their “beauty score.” The specifics on how this score was calculated will be described shortly.</p>
<p>Could it be that instructors with higher beauty scores also have higher teaching evaluations? Could it be instead that instructors with higher beauty scores tend to have lower teaching evaluations? Or could it be there is no relationship between beauty score and teaching evaluations?</p>
<p>We’ll achieve ways to address these questions by modeling the relationship between these two variables with a particular kind of linear regression called <em>simple linear regression</em>. Simple linear regression is the most basic form of linear regression. With it we have</p>
<ol style="list-style-type: decimal">
<li>A numerical outcome variable <span class="math inline">\(y\)</span>. In this case, their teaching score.</li>
<li>A single numerical explanatory variable <span class="math inline">\(x\)</span>. In this case, their beauty score.</li>
</ol>
<div id="model1EDA" class="section level3">
<h3><span class="header-section-number">1.1.1</span> 数据探索分析</h3>
<p>A crucial step before doing any kind of modeling or analysis is performing an <em>exploratory data analysis</em>, or EDA, of all our data. Exploratory data analysis can give you a sense of the distribution of the data, and whether there are outliers and/or missing values. Most importantly, it can inform how to build your model. There are many approaches to exploratory data analysis; here are three:</p>
<ol style="list-style-type: decimal">
<li>Most fundamentally: just looking at the raw values, in a spreadsheet for example. While this may seem trivial, many people ignore this crucial step!</li>
<li>Computing summary statistics likes means, medians, and standard deviations.</li>
<li>Creating data visualizations.</li>
</ol>
<p>Let’s load the data, <code>select</code> only a subset of the variables, and look at the raw values. Recall you can look at the raw values by running <code>View()</code> in the console in RStudio to pop-up the spreadsheet viewer with the data frame of interest as the argument to <code>View()</code>. Here, however, we present only a snapshot of five randomly chosen rows:</p>
<pre class="sourceCode r"><code class="sourceCode r">evals_ch6 &lt;-<span class="st"> </span>evals <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(score, bty_avg, age)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">evals_ch6 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">5</span>)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-6">Table 1.1: </span>Random sample of 5 instructors</caption>
<thead>
<tr class="header">
<th align="right">score</th>
<th align="right">bty_avg</th>
<th align="right">age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">3.6</td>
<td align="right">6.67</td>
<td align="right">34</td>
</tr>
<tr class="even">
<td align="right">4.9</td>
<td align="right">3.50</td>
<td align="right">43</td>
</tr>
<tr class="odd">
<td align="right">3.3</td>
<td align="right">2.33</td>
<td align="right">47</td>
</tr>
<tr class="even">
<td align="right">4.4</td>
<td align="right">4.67</td>
<td align="right">33</td>
</tr>
<tr class="odd">
<td align="right">4.7</td>
<td align="right">3.67</td>
<td align="right">60</td>
</tr>
</tbody>
</table>
<p>While a full description of each of these variables can be found at <a href="https://www.openintro.org/stat/data/?data=evals">openintro.org</a>, let’s summarize what each of these variables represents.</p>
<ol style="list-style-type: decimal">
<li><code>score</code>: Numerical variable of the average teaching score based on students’ evaluations between 1 and 5. This is the outcome variable <span class="math inline">\(y\)</span> of interest.</li>
<li><code>bty_avg</code>: Numerical variable of average “beauty” rating based on a panel of 6 students’ scores between 1 and 10. This is the numerical explanatory variable <span class="math inline">\(x\)</span> of interest. Here 1 corresponds to a low beauty rating and 10 to a high beauty rating.</li>
<li><code>age</code>: A numerical variable of age in years as an integer value.</li>
</ol>
<p>Another way to look at the raw values is using the <code>glimpse()</code> function, which gives us a slightly different view of the data. We see <code>Observations: 463</code>, indicating that there are 463 observations in <code>evals</code>, each corresponding to a particular instructor at UT Austin. Expressed differently, each row in the data frame <code>evals</code> corresponds to one of 463 instructors.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glimpse</span>(evals_ch6)</code></pre>
<pre><code>## Observations: 463
## Variables: 3
## $ score   &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8,...
## $ bty_avg &lt;dbl&gt; 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, ...
## $ age     &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40...</code></pre>
<p>Since both the outcome variable <code>score</code> and the explanatory variable <code>bty_avg</code> are numerical, we can compute summary statistics about them such as the mean, median, and standard deviation. Let’s take <code>evals_ch6</code> and select only the two variables of interest for now. However, let’s instead pipe this into the <code>skim()</code> function from the <code>skimr</code> package. This function quickly uses a “skim” of the data to return the following summary information about each variable.</p>
<pre class="sourceCode r"><code class="sourceCode r">evals_ch6 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(score, bty_avg) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">skim</span>()</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 463 
##  n variables: 2 
## 
## -- Variable type:numeric ---------------------------------------------------------------------------------------------------
##  variable missing complete   n mean   sd   p0  p25  p50 p75 p100     hist
##   bty_avg       0      463 463 4.42 1.53 1.67 3.17 4.33 5.5 8.17 &lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2581&gt;
##     score       0      463 463 4.17 0.54 2.3  3.8  4.3  4.6 5    &lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;</code></pre>
<p>In this case for our two numerical variables <code>bty_avg</code> beauty score and teaching score <code>score</code> it returns:</p>
<ul>
<li><code>missing</code>: the number of missing values</li>
<li><code>complete</code>: the number of non-missing or complete values</li>
<li><code>n</code>: the total number of values</li>
<li><code>mean</code>: the average</li>
<li><code>sd</code>: the standard deviation</li>
<li><code>p0</code>: the 0<sup>th</sup> percentile: the value at which 0% of observations are smaller than it. This is also known as the <em>minimum</em></li>
<li><code>p25</code>: the 25<sup>th</sup> percentile: the value at which 25% of observations are smaller than it. This is also known as the <em>1<sup>st</sup> quartile</em></li>
<li><code>p50</code>: the 25<sup>th</sup> percentile: the value at which 50% of observations are smaller than it. This is also know as the <em>2<sup>nd</sup></em> quartile and more commonly the <em>median</em></li>
<li><code>p75</code>: the 75<sup>th</sup> percentile: the value at which 75% of observations are smaller than it. This is also known as the <em>3<sup>rd</sup> quartile</em></li>
<li><code>p100</code>: the 100<sup>th</sup> percentile: the value at which 100% of observations are smaller than it. This is also known as the <em>maximum</em></li>
<li>A quick snapshot of the <code>hist</code>ogram</li>
</ul>
<p>We get an idea of how the values in both variables are distributed. For example, the mean teaching score was 4.17 out of 5 whereas the mean beauty score was 4.42 out of 10. Furthermore, the middle 50% of teaching scores were between 3.80 and 4.6 (the first and third quartiles) while the middle 50% of beauty scores were between 3.17 and 5.5 out of 10.</p>
<p>The <code>skim()</code> function however only returns what are called <em>univariate</em> summaries, i.e. summaries about single variables at a time. Since we are considering the relationship between two numerical variables, it would be nice to have a summary statistic that simultaneously considers both variables. The <em>correlation coefficient</em> is a <em>bivariate</em> summary statistic that fits this bill. <em>Coefficients</em> in general are quantitative expressions of a specific property of a phenomenon. A correlation coefficient is a quantitative expression between -1 and 1 that summarizes the <em>strength of the linear relationship between two numerical variables</em>:</p>
<ul>
<li>-1 indicates a perfect <em>negative relationship</em>: as the value of one variable goes up, the value of the other variable tends to go down.</li>
<li>0 indicates no relationship: the values of both variables go up/down independently of each other.</li>
<li>+1 indicates a perfect <em>positive relationship</em>: as the value of one variable goes up, the value of the other variable tends to go up as well.</li>
</ul>
<p>Figure <a href="#fig:correlation1">1.1</a> gives examples of different correlation coefficient values for hypothetical numerical variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We see that while for a correlation coefficient of -0.75 there is still a negative relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, it is not as strong as the negative relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> when the correlation coefficient is -1.</p>
<div class="figure"><span id="fig:correlation1"></span>
<img src="06-regression_files/figure-html/correlation1-1.png" alt="Different correlation coefficients" width="\textwidth" />
<p class="caption">
Figure 1.1: Different correlation coefficients
</p>
</div>
<p>The correlation coefficient is computed using the <code>get_correlation()</code> function in the <code>moderndive</code> package, where in this case the inputs to the function are the two numerical variables from which we want to calculate the correlation coefficient. We place the name of the response variable on the left hand side of the <code>~</code> and the explanatory variable on the right hand side of the “tilde.” We will use this same “formula” syntax with regression later in this chapter.</p>
<pre class="sourceCode r"><code class="sourceCode r">evals_ch6 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">get_correlation</span>(<span class="dt">formula =</span> score <span class="op">~</span><span class="st"> </span>bty_avg)</code></pre>
<pre><code>## # A tibble: 1 x 1
##   correlation
##         &lt;dbl&gt;
## 1       0.187</code></pre>
<p>The correlation coefficient can also be computed using the <code>cor()</code> function, where in this case the inputs to the function are the two numerical variables from which we want to calculate the correlation coefficient. Recall from Subsection <a href="#exploredataframes"><strong>??</strong></a> that the <code>$</code> pulls out specific variables from a data frame:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(<span class="dt">x =</span> evals_ch6<span class="op">$</span>bty_avg, <span class="dt">y =</span> evals_ch6<span class="op">$</span>score)</code></pre>
<pre><code>## [1] 0.187</code></pre>
<p>In our case, the correlation coefficient of 0.187 indicates that the relationship between teaching evaluation score and beauty average is “weakly positive.” There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren’t close to -1, 0, and 1. For help developing such intuition and more discussion on the correlation coefficient see Subsection <a href="#correlationcoefficient">1.3.1</a> below.</p>
<p>Let’s now proceed by visualizing this data. Since both the <code>score</code> and <code>bty_avg</code> variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let’s do this using <code>geom_point()</code> and set informative axes labels and title and display the result in Figure <a href="#fig:numxplot1">1.2</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(evals_ch6, <span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Beauty Score&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Teaching Score&quot;</span>, 
       <span class="dt">title =</span> <span class="st">&quot;Relationship of teaching and beauty scores&quot;</span>)</code></pre>
<div class="figure"><span id="fig:numxplot1"></span>
<img src="06-regression_files/figure-html/numxplot1-1.png" alt="Instructor evaluation scores at UT Austin" width="\textwidth" />
<p class="caption">
Figure 1.2: Instructor evaluation scores at UT Austin
</p>
</div>
<p>Observe the following:</p>
<ol style="list-style-type: decimal">
<li>Most “beauty” scores lie between 2 and 8.</li>
<li>Most teaching scores lie between 3 and 5.</li>
<li>Recall our earlier computation of the correlation coefficient, which describes the strength of the linear relationship between two numerical variables. Looking at Figure <a href="#fig:numxplot2">1.3</a>, it is not immediately apparent that these two variables are positively related. This is to be expected given the positive, but rather weak (close to 0), correlation coefficient of 0.187.</li>
</ol>
<p>Before we continue, we bring to light an important fact about this dataset: it suffers from <em>overplotting</em>. Recall from the data visualization Subsection <a href="#overplotting"><strong>??</strong></a> that overplotting occurs when several points are stacked directly on top of each other thereby obscuring the number of points. For example, let’s focus on the 6 points in the top-right of the plot with a beauty score of around 8 out of 10: are there truly only 6 points, or are there many more just stacked on top of each other? You can think of these as <em>ties</em>. Let’s break up these ties with a little random “jitter” added to the points in Figure <a href="#fig:numxplot2">1.3</a>.</p>
<div class="figure"><span id="fig:numxplot2"></span>
<img src="06-regression_files/figure-html/numxplot2-1.png" alt="Instructor evaluation scores at UT Austin: Jittered" width="\textwidth" />
<p class="caption">
Figure 1.3: Instructor evaluation scores at UT Austin: Jittered
</p>
</div>
<p>Jittering adds a little random bump to each of the points to break up these ties: just enough so you can distinguish them, but not so much that the plot is overly altered. Furthermore, jittering is strictly a visualization tool; it does not alter the original values in the dataset.</p>
<p>Let’s compare side-by-side the regular scatterplot in Figure <a href="#fig:numxplot1">1.2</a> with the jittered scatterplot in Figure <a href="#fig:numxplot2">1.3</a> in Figure <a href="#fig:numxplot2-a">1.4</a>.</p>
<div class="figure"><span id="fig:numxplot2-a"></span>
<img src="06-regression_files/figure-html/numxplot2-a-1.png" alt="Comparing regular and jittered scatterplots." width="\textwidth" />
<p class="caption">
Figure 1.4: Comparing regular and jittered scatterplots.
</p>
</div>
<p>We make several further observations:</p>
<!-- We might want to actually highlight these points in the plot. -->
<ol style="list-style-type: decimal">
<li>Focusing our attention on the top-right of the plot again, as noted earlier where there seemed to only be 6 points in the regular scatterplot, we see there were in fact really 9 as seen in the jittered scatterplot.</li>
<li>A further interesting trend is that the jittering revealed a large number of instructors with beauty scores of between 3 and 4.5, towards the lower end of the beauty scale.</li>
</ol>
<p>Going forward for simplicity’s sake however, we’ll only present regular scatterplot rather than the jittered scatterplots; we’ll only keep the overplotting in mind whenever looking at such plots. Going back to scatterplot in Figure <a href="#fig:numxplot1">1.2</a>, let’s improve on it by adding a “regression line” in Figure <a href="#fig:numxplot3">1.5</a>. This is easily done by adding a new layer to the <code>ggplot</code> code that created Figure <a href="#fig:numxplot2">1.3</a>: <code>+ geom_smooth(method = &quot;lm&quot;)</code>. A regression line is a “best fitting” line in that of all possible lines you could draw on this plot, it is “best” in terms of some mathematical criteria. We discuss the criteria for “best” in Subsection <a href="#leastsquares">1.3.3</a> below, but we suggest you read this only after covering the concept of a <em>residual</em> coming up in Subsection <a href="#model1points">1.1.3</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(evals_ch6, <span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Beauty Score&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Teaching Score&quot;</span>, 
       <span class="dt">title =</span> <span class="st">&quot;Relationship of teaching and beauty scores&quot;</span>) <span class="op">+</span><span class="st">  </span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)</code></pre>
<div class="figure"><span id="fig:numxplot3"></span>
<img src="06-regression_files/figure-html/numxplot3-1.png" alt="Regression line" width="\textwidth" />
<p class="caption">
Figure 1.5: Regression line
</p>
</div>
<p>When viewed on this plot, the regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable <code>score</code> and the explanatory variable <code>bty_avg</code>. The positive slope of the blue line is consistent with our observed correlation coefficient of 0.187 suggesting that there is a positive relationship between <code>score</code> and <code>bty_avg</code>. We’ll see later however that while the correlation coefficient is not equal to the slope of this line, they always have the same sign: positive or negative.</p>
<p>What are the grey bands surrounding the blue line? These are <em>standard error</em> bands, which can be thought of as error/uncertainty bands. Let’s skip this idea for now and suppress these grey bars for now by adding the argument <code>se = FALSE</code> to <code>geom_smooth(method = &quot;lm&quot;)</code>. We’ll introduce standard errors in Chapter <a href="#sampling"><strong>??</strong></a> on sampling, use them for constructing <em>confidence intervals</em> and conducting <em>hypothesis tests</em> in Chapters <a href="#confidence-intervals"><strong>??</strong></a> and <a href="#hypothesis-testing"><strong>??</strong></a>, and consider them when we revisit regression in Chapter <a href="#inference-for-regression"><strong>??</strong></a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(evals_ch6, <span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> score)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Beauty Score&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Teaching Score&quot;</span>, 
       <span class="dt">title =</span> <span class="st">&quot;Relationship of teaching and beauty scores&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre>
<div class="figure"><span id="fig:numxplot4"></span>
<img src="06-regression_files/figure-html/numxplot4-1.png" alt="Regression line without error bands" width="\textwidth" />
<p class="caption">
Figure 1.6: Regression line without error bands
</p>
</div>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC6.1)</strong> Conduct a new exploratory data analysis with the same outcome variable <span class="math inline">\(y\)</span> being <code>score</code> but with <code>age</code> as the new explanatory variable <span class="math inline">\(x\)</span>. Remember, this involves three things:</p>
<ol style="list-style-type: lower-alpha">
<li>Looking at the raw values.</li>
<li>Computing summary statistics of the variables of interest.</li>
<li>Creating informative visualizations.</li>
</ol>
<p>What can you say about the relationship between age and teaching scores based on this exploration?</p>
<div class="learncheck">

</div>
</div>
<div id="model1table" class="section level3">
<h3><span class="header-section-number">1.1.2</span> 一元线性回归</h3>
<p>You may recall from secondary school / high school algebra, in general, the equation of a line is <span class="math inline">\(y = a + bx\)</span>, which is defined by two coefficients. Recall we defined this earlier as “quantitative expressions of a specific property of a phenomenon.” These two coefficients are:</p>
<ul>
<li>the intercept coefficient <span class="math inline">\(a\)</span>, or the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span>, and</li>
<li>the slope coefficient <span class="math inline">\(b\)</span>, or the increase in <span class="math inline">\(y\)</span> for every increase of one in <span class="math inline">\(x\)</span>.</li>
</ul>
<p>However, when defining a line specifically for regression, like the blue regression line in Figure <a href="#fig:numxplot4">1.6</a>, we use slightly different notation: the equation of the regression line is <span class="math inline">\(\widehat{y} = b_0 + b_1 \cdot x\)</span> where</p>
<ul>
<li>the intercept coefficient is <span class="math inline">\(b_0\)</span>, or the value of <span class="math inline">\(\widehat{y}\)</span> when <span class="math inline">\(x=0\)</span>, and</li>
<li>the slope coefficient <span class="math inline">\(b_1\)</span>, or the increase in <span class="math inline">\(\widehat{y}\)</span> for every increase of one in <span class="math inline">\(x\)</span>.</li>
</ul>
<p>Why do we put a “hat” on top of the <span class="math inline">\(y\)</span>? It’s a form of notation commonly used in regression, which we’ll introduce in the next Subsection <a href="#model1points">1.1.3</a> when we discuss <em>fitted values</em>. For now, let’s ignore the hat and treat the equation of the line as you would from secondary school / high school algebra recognizing the slope and the intercept. We know looking at Figure <a href="#fig:numxplot4">1.6</a> that the slope coefficient corresponding to <code>bty_avg</code> should be positive. Why? Because as <code>bty_avg</code> increases, professors tend to roughly have larger teaching evaluation <code>scores</code>. However, what are the specific values of the intercept and slope coefficients? Let’s not worry about computing these by hand, but instead let the computer do the work for us. Specifically let’s use R!</p>
<p>Let’s get the value of the intercept and slope coefficients by outputting something called the <em>linear regression table</em>. We will fit the linear regression model to the <code>data</code> using the <code>lm()</code> function and save this to <code>score_model</code>. <code>lm</code> stands for “linear model”, given that we are dealing with lines. When we say “fit”, we are saying find the best fitting line to this data.</p>
<p>The <code>lm()</code> function that “fits” the linear regression model is typically used as <code>lm(y ~ x, data = data_frame_name)</code> where:</p>
<ul>
<li><code>y</code> is the outcome variable, followed by a tilde (<code>~</code>). This is likely the key to the left of “1” on your keyboard. In our case, <code>y</code> is set to <code>score</code>.</li>
<li><code>x</code> is the explanatory variable. In our case, <code>x</code> is set to <code>bty_avg</code>. We call the combination <code>y ~ x</code> a <em>model formula</em>. Recall the use of this notation when we computed the correlation coefficient using the <code>get_correlation()</code> function in Subsection <a href="#model1EDA">1.1.1</a>.</li>
<li><code>data_frame_name</code> is the name of the data frame that contains the variables <code>y</code> and <code>x</code>. In our case, <code>data_frame_name</code> is the <code>evals_ch6</code> data frame.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch6)
score_model</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ bty_avg, data = evals_ch6)
## 
## Coefficients:
## (Intercept)      bty_avg  
##      3.8803       0.0666</code></pre>
<p>This output is telling us that the <code>Intercept</code> coefficient <span class="math inline">\(b_0\)</span> of the regression line is 3.8803 and the slope coefficient for <code>by_avg</code> is 0.0666. Therefore the blue regression line in Figure <a href="#fig:numxplot4">1.6</a> is</p>
<p><span class="math display">\[\widehat{\text{score}} = b_0 + b_{\text{bty_avg}} \cdot\text{bty_avg} = 3.8803 + 0.0666\cdot\text{ bty_avg}\]</span></p>
<p>where</p>
<ul>
<li>The intercept coefficient <span class="math inline">\(b_0 = 3.8803\)</span> means for instructors that had a hypothetical beauty score of 0, we would expect them to have on average a teaching score of 3.8803. In this case however, while the intercept has a mathematical interpretation when defining the regression line, there is no <em>practical</em> interpretation since <code>score</code> is an average of a panel of 6 students’ ratings from 1 to 10, a <code>bty_avg</code> of 0 would be impossible. Furthermore, no instructors had a beauty score anywhere near 0 in this data.</li>
<li><p>Of more interest is the slope coefficient associated with <code>bty_avg</code>: <span class="math inline">\(b_{\text{bty avg}} = +0.0666\)</span>. This is a numerical quantity that summarizes the relationship between the outcome and explanatory variables. Note that the sign is positive, suggesting a positive relationship between beauty scores and teaching scores, meaning as beauty scores go up, so also do teaching scores go up. The slope’s precise interpretation is:</p>
<blockquote>
<p>For every increase of 1 unit in <code>bty_avg</code>, there is an <em>associated</em> increase of, <em>on average</em>, 0.0666 units of <code>score</code>.</p>
</blockquote></li>
</ul>
<p>Such interpretations need be carefully worded:</p>
<ul>
<li>We only stated that there is an <em>associated</em> increase, and not necessarily a <em>causal</em> increase. For example, perhaps it’s not that beauty directly affects teaching scores, but instead individuals from wealthier backgrounds tend to have had better education and training, and hence have higher teaching scores, but these same individuals also have higher beauty scores. Avoiding such reasoning can be summarized by the adage “correlation is not necessarily causation.” In other words, just because two variables are correlated, it doesn’t mean one directly causes the other. We discuss these ideas more in Subsection <a href="#correlation-is-not-causation">1.3.2</a>.<br />
</li>
<li>We say that this associated increase is <em>on average</em> 0.0666 units of teaching <code>score</code> and not that the associated increase is <em>exactly</em> 0.0666 units of <code>score</code> across all values of <code>bty_avg</code>. This is because the slope is the average increase across all points as shown by the regression line in Figure <a href="#fig:numxplot4">1.6</a>.</li>
</ul>
<p>Now that we’ve learned how to compute the equation for the blue regression line in Figure <a href="#fig:numxplot4">1.6</a> and interpreted all its terms, let’s take our modeling one step further. This time after fitting the model using the <code>lm()</code>, let’s get something called the <em>regression table</em> using the <code>get_regression_table()</code> function from the <code>moderndive</code> package:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit regression model:</span>
score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch6)
<span class="co"># Get regression table:</span>
<span class="kw">get_regression_table</span>(score_model)</code></pre>
<table>
<caption><span id="tab:numxplot4b">Table 1.2: </span>Linear regression table</caption>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std_error</th>
<th align="right">statistic</th>
<th align="right">p_value</th>
<th align="right">lower_ci</th>
<th align="right">upper_ci</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">intercept</td>
<td align="right">3.880</td>
<td align="right">0.076</td>
<td align="right">50.96</td>
<td align="right">0</td>
<td align="right">3.731</td>
<td align="right">4.030</td>
</tr>
<tr class="even">
<td align="left">bty_avg</td>
<td align="right">0.067</td>
<td align="right">0.016</td>
<td align="right">4.09</td>
<td align="right">0</td>
<td align="right">0.035</td>
<td align="right">0.099</td>
</tr>
</tbody>
</table>
<p>Note how we took the output of the model fit saved in <code>score_model</code> and used it as an input to the subsequent <code>get_regression_table()</code> function. The output now looks like a table: in fact it is a data frame. The values of the intercept and slope of 3.880 and 0.0666 are now in the <code>estimate</code> column. But what are the remaining 5 columns: <code>std_error</code>, <code>statistic</code>, <code>p_value</code>, <code>lower_ci</code> and <code>upper_ci</code>? What do they tell us? They tell us about both the <em>statistical significance</em> and <em>practical significance</em> of our model results. You can think of this loosely as the “meaningfulness” of the results from a statistical perspective.</p>
<p>We are going to put aside these ideas for now and revisit them in Chapter <a href="#inference-for-regression"><strong>??</strong></a> on (statistical) inference for regression, after we’ve had a chance to cover:</p>
<ul>
<li>Standard errors in Chapter <a href="#sampling"><strong>??</strong></a> (<code>std_error</code>)</li>
<li>Confidence intervals in Chapter <a href="#confidence-intervals"><strong>??</strong></a> (<code>lower_ci</code> and <code>upper_ci</code>)</li>
<li>Hypothesis testing in Chapter <a href="#hypothesis-testing"><strong>??</strong></a> (<code>statistic</code> and <code>p_value</code>).</li>
</ul>
<p>For now, we’ll only focus on the <code>term</code> and <code>estimate</code> columns of any regression table.</p>
<p>The <code>get_regression_table()</code> from the <code>moderndive</code> is an example of what’s known as a <em>wrapper function</em> in computer programming, which takes other pre-existing functions and “wraps” them into a single function. This concept is illustrated in Figure <a href="#fig:moderndive-figure-wrapper">1.7</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:moderndive-figure-wrapper"></span>
<img src="images/flowcharts/flowchart.011-cropped.png" alt="The concept of a 'wrapper' function." width="\textwidth" />
<p class="caption">
Figure 1.7: The concept of a ‘wrapper’ function.
</p>
</div>
<p>So all you need to worry about is the what the inputs look like and what the outputs look like; you leave all the other details “under the hood of the car.” In our regression modeling example, the <code>get_regression_table()</code> has</p>
<ul>
<li>Input: A saved <code>lm()</code> linear regression</li>
<li>Output: A data frame with information on the intercept and slope of the regression line.</li>
</ul>
<p>If you’re interested in learning more about the <code>get_regression_table()</code> function’s construction and thinking, see Subsection <a href="#underthehood">1.3.4</a> below.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC6.2)</strong> Fit a new simple linear regression using <code>lm(score ~ age, data = evals_ch6)</code> where <code>age</code> is the new explanatory variable <span class="math inline">\(x\)</span>. Get information about the “best-fitting” line from the regression table by applying the <code>get_regression_table()</code> function. How do the regression results match up with the results from your exploratory data analysis above?</p>
<div class="learncheck">

</div>
</div>
<div id="model1points" class="section level3">
<h3><span class="header-section-number">1.1.3</span> 观察值,拟合值,残差</h3>
<p>We just saw how to get the value of the intercept and the slope of the regression line from the regression table generated by <code>get_regression_table()</code>. Now instead, say we want information on individual points. In this case, we focus on one of the <span class="math inline">\(n = 463\)</span> instructors in this dataset, corresponding to a single row of <code>evals_ch6</code>.</p>
<p>For example, say we are interested in the 21st instructor in this dataset:</p>
<table>
<caption><span id="tab:unnamed-chunk-18">Table 1.3: </span>Data for 21st instructor</caption>
<thead>
<tr class="header">
<th align="right">score</th>
<th align="right">bty_avg</th>
<th align="right">age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4.9</td>
<td align="right">7.33</td>
<td align="right">31</td>
</tr>
</tbody>
</table>
<p>What is the value on the blue line corresponding to this instructor’s <code>bty_avg</code> of 7.333? In Figure <a href="#fig:numxplot5">1.8</a> we mark three values in particular corresponding to this instructor.</p>
<ul>
<li>Red circle: This is the <em>observed value</em> <span class="math inline">\(y\)</span> = 4.9 and corresponds to this instructor’s actual teaching score.</li>
<li>Red square: This is the <em>fitted value</em> <span class="math inline">\(\widehat{y}\)</span> and corresponds to the value on the regression line for <span class="math inline">\(x\)</span> = 7.333. This value is computed using the intercept and slope in the regression table above: <span class="math display">\[\widehat{y} = b_0 + b_1 \cdot x = 3.88 + 0.067 * 7.333 = 4.369\]</span></li>
<li>Blue arrow: The length of this arrow is the <em>residual</em> and is computed by subtracting the fitted value <span class="math inline">\(\widehat{y}\)</span> from the observed value <span class="math inline">\(y\)</span>. The residual can be thought of as the error or “lack of fit” of the regression line. In the case of this instructor, it is <span class="math inline">\(y - \widehat{y}\)</span> = 4.9 - 4.369 = 0.531. In other words, the model was off by 0.531 teaching score units for this instructor.</li>
</ul>
<div class="figure"><span id="fig:numxplot5"></span>
<img src="06-regression_files/figure-html/numxplot5-1.png" alt="Example of observed value, fitted value, and residual" width="\textwidth" />
<p class="caption">
Figure 1.8: Example of observed value, fitted value, and residual
</p>
</div>
<p>What if we want both</p>
<ol style="list-style-type: decimal">
<li>the fitted value <span class="math inline">\(\widehat{y} = b_0 + b_1 \cdot x\)</span> and</li>
<li>the residual <span class="math inline">\(y - \widehat{y}\)</span></li>
</ol>
<p>not only the 21st instructor but for all 463 instructors in the study? Recall that each instructor corresponds to one of the 463 rows in the <code>evals_ch6</code> data frame and also one of the 463 points in the regression plot in Figure <a href="#fig:numxplot4">1.6</a>.</p>
<p>We could repeat the above calculations by hand 463 times, but that would be tedious and time consuming. Instead, let’s use the <code>get_regression_points()</code> function that we’ve included in the <code>moderndive</code> R package. Note that in the table below we only present the results for the 21st through the 24th instructors.</p>
<pre class="sourceCode r"><code class="sourceCode r">regression_points &lt;-<span class="st"> </span><span class="kw">get_regression_points</span>(score_model)
regression_points</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-21">Table 1.4: </span>Regression points (for only 21st through 24th instructor)</caption>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="right">score</th>
<th align="right">bty_avg</th>
<th align="right">score_hat</th>
<th align="right">residual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">21</td>
<td align="right">4.9</td>
<td align="right">7.33</td>
<td align="right">4.37</td>
<td align="right">0.531</td>
</tr>
<tr class="even">
<td align="right">22</td>
<td align="right">4.6</td>
<td align="right">7.33</td>
<td align="right">4.37</td>
<td align="right">0.231</td>
</tr>
<tr class="odd">
<td align="right">23</td>
<td align="right">4.5</td>
<td align="right">7.33</td>
<td align="right">4.37</td>
<td align="right">0.131</td>
</tr>
<tr class="even">
<td align="right">24</td>
<td align="right">4.4</td>
<td align="right">5.50</td>
<td align="right">4.25</td>
<td align="right">0.153</td>
</tr>
</tbody>
</table>
<p>Just as with the <code>get_regression_table()</code> function, the inputs to the <code>get_regression_points()</code> function are the same, however the outputs are different. Let’s inspect the individual columns:</p>
<ul>
<li>The <code>score</code> column represents the observed value of the outcome variable <span class="math inline">\(y\)</span>.</li>
<li>The <code>bty_avg</code> column represents the values of the explanatory variable <span class="math inline">\(x\)</span>.</li>
<li>The <code>score_hat</code> column represents the fitted values <span class="math inline">\(\widehat{y}\)</span>.</li>
<li>The <code>residual</code> column represents the residuals <span class="math inline">\(y - \widehat{y}\)</span>.</li>
</ul>
<p><code>get_regression_points()</code> is another example of a wrapper function we described in Figure <a href="#fig:moderndive-figure-wrapper">1.7</a>. If you’re curious about this function as well, check out Subsection <a href="#underthehood">1.3.4</a>.</p>
<p>Just as we did for the 21st instructor in the <code>evals_ch6</code> dataset (in the first row of the table above), let’s repeat the above calculations for the 24th instructor in the <code>evals_ch6</code> dataset (in the fourth row of the table above):</p>
<ul>
<li><code>score</code> = 4.4 is the observed value <span class="math inline">\(y\)</span> for this instructor.</li>
<li><code>bty_avg</code> = 5.50 is the value of the explanatory variable <span class="math inline">\(x\)</span> for this instructor.</li>
<li><code>score_hat</code> = 4.25 = 3.88 + 0.067 * <span class="math inline">\(x\)</span> = 3.88 + 0.067 * 5.50 is the fitted value <span class="math inline">\(\widehat{y}\)</span> for this instructor.</li>
<li><code>residual</code> = 0.153 = 4.4 - 4.25 is the value of the residual for this instructor. In other words, the model was off by 0.153 teaching score units for this instructor.</li>
</ul>
<p>More development of this idea appears in Section <a href="#leastsquares">1.3.3</a> and we encourage you to read that section after you investigate residuals.</p>
</div>
<div id="model1residuals" class="section level3">
<h3><span class="header-section-number">1.1.4</span> 残差分析</h3>
<p>Recall the residuals can be thought of as the error or the “lack-of-fit” between the observed value <span class="math inline">\(y\)</span> and the fitted value <span class="math inline">\(\widehat{y}\)</span> on the blue regression line in Figure <a href="#fig:numxplot4">1.6</a>. Ideally when we fit a regression model, we’d like there to be <em>no systematic pattern</em> to these residuals. We’ll be more specific as to what we mean by <em>no systematic pattern</em> when we see Figure <a href="#fig:numxplot7">1.10</a> below, but let’s keep this notion imprecise for now. Investigating any such patterns is known as <em>residual analysis</em> and is the theme of this section.</p>
<p>We’ll perform our residual analysis in two ways:</p>
<ol style="list-style-type: decimal">
<li>Creating a scatterplot with the residuals on the <span class="math inline">\(y\)</span>-axis and the original explanatory variable <span class="math inline">\(x\)</span> on the <span class="math inline">\(x\)</span>-axis.</li>
<li>Creating a histogram of the residuals, thereby showing the <em>distribution</em> of the residuals.</li>
</ol>
<p>First, recall in Figure <a href="#fig:numxplot5">1.8</a> above we created a scatterplot where</p>
<ul>
<li>on the vertical axis we had the teaching score <span class="math inline">\(y\)</span>,</li>
<li>on the horizontal axis we had the beauty score <span class="math inline">\(x\)</span>, and</li>
<li>the blue arrow represented the residual for one particular instructor.</li>
</ul>
<p>Instead, in Figure <a href="#fig:numxplot6">1.9</a> below, let’s create a scatterplot where</p>
<ul>
<li>On the vertical axis we have the residual <span class="math inline">\(y-\widehat{y}\)</span> instead</li>
<li>On the horizontal axis we have the beauty score <span class="math inline">\(x\)</span> as before:</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(regression_points, <span class="kw">aes</span>(<span class="dt">x =</span> bty_avg, <span class="dt">y =</span> residual)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Beauty Score&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Residual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>)</code></pre>
<div class="figure"><span id="fig:numxplot6"></span>
<img src="06-regression_files/figure-html/numxplot6-1.png" alt="Plot of residuals over beauty score" width="\textwidth" />
<p class="caption">
Figure 1.9: Plot of residuals over beauty score
</p>
</div>
<p>You can think of Figure <a href="#fig:numxplot6">1.9</a> as Figure <a href="#fig:numxplot5">1.8</a> but with the blue line flattened out to <span class="math inline">\(y=0\)</span>. Does it seem like there is <em>no systematic pattern</em> to the residuals? This question is rather qualitative and subjective in nature, thus different people may respond with different answers to the above question. However, it can be argued that there isn’t a <em>drastic</em> pattern in the residuals.</p>
<p>Let’s now get a little more precise in our definition of <em>no systematic pattern</em> in the residuals. Ideally, the residuals should behave <em>randomly</em>. In addition,</p>
<ol style="list-style-type: decimal">
<li>the residuals should be on average 0. In other words, sometimes the regression model will make a positive error in that <span class="math inline">\(y - \widehat{y} &gt; 0\)</span>, sometimes the regression model will make a negative error in that <span class="math inline">\(y - \widehat{y} &lt; 0\)</span>, but <em>on average</em> the error is 0.</li>
<li>Further, the value and spread of the residuals should not depend on the value of <span class="math inline">\(x\)</span>.</li>
</ol>
<p>In Figure <a href="#fig:numxplot7">1.10</a> below, we display some hypothetical examples where there are <em>drastic</em> patterns to the residuals. In Example 1, the value of the residual seems to depend on <span class="math inline">\(x\)</span>: the residuals tend to be positive for small and large values of <span class="math inline">\(x\)</span> in this range, whereas values of <span class="math inline">\(x\)</span> more in the middle tend to have negative residuals. In Example 2, while the residuals seem to be on average 0 for each value of <span class="math inline">\(x\)</span>, the spread of the residuals varies for different values of <span class="math inline">\(x\)</span>; this situation is known as <em>heteroskedasticity</em>.</p>
<div class="figure"><span id="fig:numxplot7"></span>
<img src="06-regression_files/figure-html/numxplot7-1.png" alt="Examples of less than ideal residual patterns" width="\textwidth" />
<p class="caption">
Figure 1.10: Examples of less than ideal residual patterns
</p>
</div>
<p>The second way to perform a residual analysis is to look at the histogram of the residuals:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(regression_points, <span class="kw">aes</span>(<span class="dt">x =</span> residual)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="fl">0.25</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Residual&quot;</span>)</code></pre>
<div class="figure"><span id="fig:model1residualshist"></span>
<img src="06-regression_files/figure-html/model1residualshist-1.png" alt="Histogram of residuals" width="\textwidth" />
<p class="caption">
Figure 1.11: Histogram of residuals
</p>
</div>
<p>This histogram seems to indicate that we have more positive residuals than negative. Since the residual <span class="math inline">\(y-\widehat{y}\)</span> is positive when <span class="math inline">\(y &gt; \widehat{y}\)</span>, it seems our fitted teaching score from the regression model tends to <em>underestimate</em> the true teaching score. This histogram has a slight <em>left-skew</em> in that there is a long tail on the left. Another way to say this is this data exhibits a <em>negative skew</em>. Is this a problem? Again, there is a certain amount of subjectivity in the response. In the authors’ opinion, while there is a slight skew/pattern to the residuals, it isn’t a large concern. On the other hand, others might disagree with our assessment. Here are examples of an ideal and less than ideal pattern to the residuals when viewed in a histogram:</p>
<div class="figure"><span id="fig:numxplot9"></span>
<img src="06-regression_files/figure-html/numxplot9-1.png" alt="Examples of ideal and less than ideal residual patterns" width="\textwidth" />
<p class="caption">
Figure 1.12: Examples of ideal and less than ideal residual patterns
</p>
</div>
<p>In fact, we’ll see later on that we would like the residuals to be <em>normally distributed</em> with
mean 0. In other words, be bell-shaped and centered at 0! While this requirement and residual analysis in general may seem to some of you as not being overly critical at this point, we’ll see later after when we cover <em>inference for regression</em> in Chapter <a href="#inference-for-regression"><strong>??</strong></a> that for the last five columns of the regression table from earlier (<code>std error</code>, <code>statistic</code>, <code>p_value</code>,<code>lower_ci</code>, and <code>upper_ci</code>) to have valid interpretations, the above three conditions should roughly hold.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC6.3)</strong> Continuing with our regression using <code>age</code> as the explanatory variable and teaching <code>score</code> as the outcome variable, use the <code>get_regression_points()</code> function to get the observed values, fitted values, and residuals for all 463 instructors. Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern.</p>
<div class="learncheck">

</div>
</div>
</div>
<div id="model2" class="section level2">
<h2><span class="header-section-number">1.2</span> 单个类别型解释变量</h2>
<p>It’s an unfortunate truth that life expectancy is not the same across various countries in the world; there are a multitude of factors that are associated with how long people live. International development agencies are very interested in studying these differences in the hope of understanding where governments should allocate resources to address this problem. In this section, we’ll explore differences in life expectancy in two ways:</p>
<ol style="list-style-type: decimal">
<li>Differences between continents: Are there significant differences in life expectancy, on average, between the five continents of the world: Africa, the Americas, Asia, Europe, and Oceania?</li>
<li>Differences within continents: How does life expectancy vary within the world’s five continents? For example, is the spread of life expectancy among the countries of Africa larger than the spread of life expectancy among the countries of Asia?</li>
</ol>
<p>To answer such questions, we’ll study the <code>gapminder</code> dataset in the <code>gapminder</code> package. Recall we mentioned this dataset in Subsection <a href="#gapminder"><strong>??</strong></a> when we first studied the “Grammar of Graphics” introduced in Figure <a href="#fig:gapminder"><strong>??</strong></a>. This dataset has international development statistics such as life expectancy, GDP per capita, and population by country (<span class="math inline">\(n\)</span> = 142) for 5-year intervals between 1952 and 2007.</p>
<p>We’ll use this data for linear regression again, but note that our explanatory variable <span class="math inline">\(x\)</span> is now categorical, and not numerical like when we covered simple linear regression in Section <a href="#model1">1.1</a>. More precisely, we have:</p>
<ol style="list-style-type: decimal">
<li>A numerical outcome variable <span class="math inline">\(y\)</span>. In this case, life expectancy.</li>
<li>A single categorical explanatory variable <span class="math inline">\(x\)</span>, In this case, the continent the country is part of.</li>
</ol>
<p>When the explanatory variable <span class="math inline">\(x\)</span> is categorical, the concept of a “best-fitting” line is a little different than the one we saw previously in Section <a href="#model1">1.1</a> where the explanatory variable <span class="math inline">\(x\)</span> was numerical. We’ll study these differences shortly in Subsection <a href="#model2table">1.2.2</a>, but first we conduct our exploratory data analysis.</p>
<div id="model2EDA" class="section level3">
<h3><span class="header-section-number">1.2.1</span> 数据探索分析</h3>
<p>Let’s load the <code>gapminder</code> data and <code>filter()</code> for only observations in 2007. Next we <code>select()</code> only the variables we’ll need along with <code>gdpPercap</code>, which is each country’s gross domestic product per capita (GDP). GDP is a rough measure of that country’s economic performance. (This will be used for the upcoming Learning Check). Lastly, we save this in a data frame with name <code>gapminder2007</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gapminder)
gapminder2007 &lt;-<span class="st"> </span>gapminder <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(year <span class="op">==</span><span class="st"> </span><span class="dv">2007</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(country, continent, lifeExp, gdpPercap)</code></pre>
<p>You should look at the raw data values both by bringing up RStudio’s spreadsheet viewer and the <code>glimpse()</code> function. In Table <a href="#tab:model2-data-preview">1.5</a> we only show 5 randomly selected countries out of 142:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">View</span>(gapminder2007)</code></pre>
<table>
<caption><span id="tab:model2-data-preview">Table 1.5: </span>Random sample of 5 countries</caption>
<thead>
<tr class="header">
<th align="left">country</th>
<th align="left">continent</th>
<th align="right">lifeExp</th>
<th align="right">gdpPercap</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Slovak Republic</td>
<td align="left">Europe</td>
<td align="right">74.7</td>
<td align="right">18678</td>
</tr>
<tr class="even">
<td align="left">Israel</td>
<td align="left">Asia</td>
<td align="right">80.7</td>
<td align="right">25523</td>
</tr>
<tr class="odd">
<td align="left">Bulgaria</td>
<td align="left">Europe</td>
<td align="right">73.0</td>
<td align="right">10681</td>
</tr>
<tr class="even">
<td align="left">Tanzania</td>
<td align="left">Africa</td>
<td align="right">52.5</td>
<td align="right">1107</td>
</tr>
<tr class="odd">
<td align="left">Myanmar</td>
<td align="left">Asia</td>
<td align="right">62.1</td>
<td align="right">944</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glimpse</span>(gapminder2007)</code></pre>
<pre><code>## Observations: 142
## Variables: 4
## $ country   &lt;fct&gt; Afghanistan, Albania, Algeria, Angola, Argentina, Au...
## $ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Eur...
## $ lifeExp   &lt;dbl&gt; 43.8, 76.4, 72.3, 42.7, 75.3, 81.2, 79.8, 75.6, 64.1...
## $ gdpPercap &lt;dbl&gt; 975, 5937, 6223, 4797, 12779, 34435, 36126, 29796, 1...</code></pre>
<p>We see that the variable <code>continent</code> is indeed categorical, as it is encoded as <code>fct</code> which stands for “factor.” This is R’s way of storing categorical variables. Let’s once again apply the <code>skim()</code> function from the <code>skimr</code> package to our two variables of interest: <code>continent</code> and <code>lifeExp</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">gapminder2007 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(continent, lifeExp) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">skim</span>()</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 142 
##  n variables: 2 
## 
## -- Variable type:factor ----------------------------------------------------------------------------------------------------
##   variable missing complete   n n_unique
##  continent       0      142 142        5
##                          top_counts ordered
##  Afr: 52, Asi: 33, Eur: 30, Ame: 25   FALSE
## 
## -- Variable type:numeric ---------------------------------------------------------------------------------------------------
##  variable missing complete   n  mean    sd    p0   p25   p50   p75 p100
##   lifeExp       0      142 142 67.01 12.07 39.61 57.16 71.94 76.41 82.6
##      hist
##  &lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2587&gt;</code></pre>
<p>The output now reports summaries for categorical variables (the variable type: factor) separately from the numerical variables. For the categorical variable <code>continent</code> it now reports:</p>
<ul>
<li><code>missing</code>, <code>complete</code>, <code>n</code> as before which are the number of missing, complete, and total number of values.</li>
<li><code>n_unique</code>: The unique number of levels to this variable, corresponding to Africa, Asia, Americas, Europe, and Oceania</li>
<li><code>top_counts</code>: In this case the top four counts: Africa has 52 entries each corresponding to a country, Asia has 33, Europe has 30, and Americans has 25. Not displayed is Oceania with 2 countries</li>
<li><code>ordered</code>: Reporting whether the variable is “ordinal.” In this case, it is not ordered.</li>
</ul>
<p>Given that the global median life expectancy is 71.94, half of the world’s countries (71 countries) will have a life expectancy less than 71.94. Further, half will have a life expectancy greater than this value. The mean life expectancy of 67.01 is lower however. Why are these two values different? Let’s look at a histogram of <code>lifeExp</code> in Figure <a href="#fig:lifeExp2007hist">1.13</a> to see why.</p>
<div class="figure"><span id="fig:lifeExp2007hist"></span>
<img src="06-regression_files/figure-html/lifeExp2007hist-1.png" alt="Histogram of Life Expectancy in 2007" width="\textwidth" />
<p class="caption">
Figure 1.13: Histogram of Life Expectancy in 2007
</p>
</div>
<p>We see that this data is left-skewed/negatively skewed: there are a few countries with very low life expectancies that are bringing down the mean life expectancy. However, the median is less sensitive to the effects of such outliers. Hence the median is greater than the mean in this case. Let’s proceed by comparing median and mean life expectancy between continents by adding a <code>group_by(continent)</code> to the above code:</p>
<pre class="sourceCode r"><code class="sourceCode r">lifeExp_by_continent &lt;-<span class="st"> </span>gapminder2007 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(continent) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">median =</span> <span class="kw">median</span>(lifeExp), <span class="dt">mean =</span> <span class="kw">mean</span>(lifeExp))</code></pre>
<table>
<caption><span id="tab:catxplot0">Table 1.6: </span>Life expectancy by continent</caption>
<thead>
<tr class="header">
<th align="left">continent</th>
<th align="right">median</th>
<th align="right">mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Africa</td>
<td align="right">52.9</td>
<td align="right">54.8</td>
</tr>
<tr class="even">
<td align="left">Americas</td>
<td align="right">72.9</td>
<td align="right">73.6</td>
</tr>
<tr class="odd">
<td align="left">Asia</td>
<td align="right">72.4</td>
<td align="right">70.7</td>
</tr>
<tr class="even">
<td align="left">Europe</td>
<td align="right">78.6</td>
<td align="right">77.6</td>
</tr>
<tr class="odd">
<td align="left">Oceania</td>
<td align="right">80.7</td>
<td align="right">80.7</td>
</tr>
</tbody>
</table>
<p>We see now that there are differences in life expectancies between the continents. For example let’s focus on only medians. While the median life expectancy across all <span class="math inline">\(n = 142\)</span> countries in 2007 was 71.935, the median life expectancy across the <span class="math inline">\(n =52\)</span> countries in Africa was only 52.927.</p>
<p>Let’s create a corresponding visualization. One way to compare the life expectancies of countries in different continents would be via a faceted histogram. Recall we saw back in the Data Visualization chapter, specifically Section <a href="#facets"><strong>??</strong></a>, that facets allow us to split a visualization by the different levels of a categorical variable or factor variable. In Figure <a href="#fig:catxplot0b">1.14</a>, the variable we facet by is <code>continent</code>, which is categorical with five levels, each corresponding to the five continents of the world.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(gapminder2007, <span class="kw">aes</span>(<span class="dt">x =</span> lifeExp)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">5</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Life expectancy&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Number of countries&quot;</span>, 
       <span class="dt">title =</span> <span class="st">&quot;Life expectancy by continent&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>continent, <span class="dt">nrow =</span> <span class="dv">2</span>)</code></pre>
<div class="figure"><span id="fig:catxplot0b"></span>
<img src="06-regression_files/figure-html/catxplot0b-1.png" alt="Life expectancy in 2007" width="\textwidth" />
<p class="caption">
Figure 1.14: Life expectancy in 2007
</p>
</div>
<p>Another way would be via a <code>geom_boxplot</code> where we map the categorical variable <code>continent</code> to the <span class="math inline">\(x\)</span>-axis and the different life expectancies within each continent on the <span class="math inline">\(y\)</span>-axis; we do this in Figure <a href="#fig:catxplot1">1.15</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(gapminder2007, <span class="kw">aes</span>(<span class="dt">x =</span> continent, <span class="dt">y =</span> lifeExp)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Continent&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Life expectancy (years)&quot;</span>, 
       <span class="dt">title =</span> <span class="st">&quot;Life expectancy by continent&quot;</span>) </code></pre>
<div class="figure"><span id="fig:catxplot1"></span>
<img src="06-regression_files/figure-html/catxplot1-1.png" alt="Life expectancy in 2007" width="\textwidth" />
<p class="caption">
Figure 1.15: Life expectancy in 2007
</p>
</div>
<p>Some people prefer comparing a numerical variable between different levels of a categorical variable, in this case comparing life expectancy between different continents, using a boxplot over a faceted histogram as we can make quick comparisons with single horizontal lines. For example, we can see that even the country with the highest life expectancy in Africa is still lower than all countries in Oceania.</p>
<p>It’s important to remember however that the solid lines in the middle of the boxes correspond to the medians (i.e. the middle value) rather than the mean (the average). So, for example, if you look at Asia, the solid line denotes the median life expectancy of around 72 years, indicating to us that half of all countries in Asia have a life expectancy below 72 years whereas half of all countries in Asia have a life expectancy above 72 years. Furthermore, note that:</p>
<ul>
<li>Africa and Asia have much more spread/variation in life expectancy as indicated by the interquartile range (the height of the boxes).</li>
<li>Oceania has almost no spread/variation, but this might in large part be due to the fact there are only two countries in Oceania: Australia and New Zealand.</li>
</ul>
<p>Now, let’s start making comparisons of life expectancy <em>between</em> continents. Let’s use Africa as a <em>baseline for comparsion</em>. Why Africa? Only because it happened to be first alphabetically, we could have just as appropriately used the Americas as the baseline for comparison. Using the “eyeball test” (just using our eyes to see if anything stands out), we make the following observations about differences in median life expectancy compared to the baseline of Africa:</p>
<ol style="list-style-type: decimal">
<li>The median life expectancy of the Americas is roughly 20 years greater.</li>
<li>The median life expectancy of Asia is roughly 20 years greater.</li>
<li>The median life expectancy of Europe is roughly 25 years greater.</li>
<li>The median life expectancy of Oceania is roughly 27.8 years greater.</li>
</ol>
<p>Let’s remember these four differences vs Africa corresponding to the Americas, Asia, Europe, and Oceania: 20, 20, 25, 27.8.</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC6.4)</strong> Conduct a new exploratory data analysis with the same explanatory variable <span class="math inline">\(x\)</span> being <code>continent</code> but with <code>gdpPercap</code> as the new outcome variable <span class="math inline">\(y\)</span>. Remember, this involves three things:</p>
<ol style="list-style-type: lower-alpha">
<li>Looking at the raw values</li>
<li>Computing summary statistics of the variables of interest.</li>
<li>Creating informative visualizations</li>
</ol>
<p>What can you say about the differences in GDP per capita between continents based on this exploration?</p>
<div class="learncheck">

</div>
</div>
<div id="model2table" class="section level3">
<h3><span class="header-section-number">1.2.2</span> 线性回归</h3>
<p>In Subsection <a href="#model1table">1.1.2</a> we introduced <em>simple</em> linear regression, which involves modeling the relationship between a numerical outcome variable <span class="math inline">\(y\)</span> as a function of a numerical explanatory variable <span class="math inline">\(x\)</span>, in our life expectancy example, we now have a categorical explanatory variable <span class="math inline">\(x\)</span> <code>continent</code>. While we still can fit a regression model, given our categorical explanatory variable we no longer have a concept of a “best-fitting” line, but rather “differences relative to a baseline for comparison.”</p>
<p>Before we fit our regression model, let’s create a table similar to Table <a href="#tab:catxplot0">1.6</a>, but</p>
<ol style="list-style-type: decimal">
<li>Report the mean life expectancy for each continent.</li>
<li>Report the difference in mean life expectancy <em>relative</em> to Africa’s mean life expectancy of 54.806 in the column “mean vs Africa”; this column is simply the “mean” column minus 54.806.</li>
</ol>
<p>Think back to your observations from the eyeball test of Figure <a href="#fig:catxplot1">1.15</a> at the end of the last subsection. The column “mean vs Africa” is the same idea of comparing a summary statistic to a baseline for comparison, in this case the countries of Africa, but using means instead of medians.</p>
<table>
<caption><span id="tab:continent-mean-life-expectancies">Table 1.7: </span>Mean life expectancy by continent</caption>
<thead>
<tr class="header">
<th align="left">continent</th>
<th align="right">mean</th>
<th align="right">mean vs Africa</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Africa</td>
<td align="right">54.8</td>
<td align="right">0.0</td>
</tr>
<tr class="even">
<td align="left">Americas</td>
<td align="right">73.6</td>
<td align="right">18.8</td>
</tr>
<tr class="odd">
<td align="left">Asia</td>
<td align="right">70.7</td>
<td align="right">15.9</td>
</tr>
<tr class="even">
<td align="left">Europe</td>
<td align="right">77.6</td>
<td align="right">22.8</td>
</tr>
<tr class="odd">
<td align="left">Oceania</td>
<td align="right">80.7</td>
<td align="right">25.9</td>
</tr>
</tbody>
</table>
<p>Now, let’s use the <code>get_regression_table()</code> function we introduced in Section <a href="#model1table">1.1.2</a> to get the <em>regression table</em> for <code>gapminder2007</code> analysis:</p>
<pre class="sourceCode r"><code class="sourceCode r">lifeExp_model &lt;-<span class="st"> </span><span class="kw">lm</span>(lifeExp <span class="op">~</span><span class="st"> </span>continent, <span class="dt">data =</span> gapminder2007)
<span class="kw">get_regression_table</span>(lifeExp_model)</code></pre>
<table>
<caption><span id="tab:catxplot4b">Table 1.8: </span>Linear regression table</caption>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std_error</th>
<th align="right">statistic</th>
<th align="right">p_value</th>
<th align="right">lower_ci</th>
<th align="right">upper_ci</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">intercept</td>
<td align="right">54.8</td>
<td align="right">1.02</td>
<td align="right">53.45</td>
<td align="right">0</td>
<td align="right">52.8</td>
<td align="right">56.8</td>
</tr>
<tr class="even">
<td align="left">continentAmericas</td>
<td align="right">18.8</td>
<td align="right">1.80</td>
<td align="right">10.45</td>
<td align="right">0</td>
<td align="right">15.2</td>
<td align="right">22.4</td>
</tr>
<tr class="odd">
<td align="left">continentAsia</td>
<td align="right">15.9</td>
<td align="right">1.65</td>
<td align="right">9.68</td>
<td align="right">0</td>
<td align="right">12.7</td>
<td align="right">19.2</td>
</tr>
<tr class="even">
<td align="left">continentEurope</td>
<td align="right">22.8</td>
<td align="right">1.70</td>
<td align="right">13.47</td>
<td align="right">0</td>
<td align="right">19.5</td>
<td align="right">26.2</td>
</tr>
<tr class="odd">
<td align="left">continentOceania</td>
<td align="right">25.9</td>
<td align="right">5.33</td>
<td align="right">4.86</td>
<td align="right">0</td>
<td align="right">15.4</td>
<td align="right">36.5</td>
</tr>
</tbody>
</table>
<p>Just as before, we have the <code>term</code> and <code>estimates</code> columns of interest, but unlike before, we now have 5 rows corresponding to 5 outputs in our table: an intercept like before, but also <code>continentAmericas</code>, <code>continentAsia</code>, <code>continentEurope</code>, and <code>continentOceania</code>. What are these values? First, we must describe the equation for fitted value <span class="math inline">\(\widehat{y}\)</span>, which is a little more complicated when the <span class="math inline">\(x\)</span> explanatory variable is categorical:</p>
<p><span class="math display">\[
\begin{align}
\widehat{\text{life exp}} &amp;= b_0 + b_{\text{Amer}}\cdot\mathbb{1}_{\mbox{Amer}}(x) + b_{\text{Asia}}\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ b_{\text{Euro}}\cdot\mathbb{1}_{\mbox{Euro}}(x) + b_{\text{Ocean}}\cdot\mathbb{1}_{\mbox{Ocean}}(x)\\
&amp;= 54.8 + 18.8\cdot\mathbb{1}_{\mbox{Amer}}(x) + 15.9\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ 22.8\cdot\mathbb{1}_{\mbox{Euro}}(x) + 25.9\cdot\mathbb{1}_{\mbox{Ocean}}(x)
\end{align}
\]</span></p>
<p>Let’s break this down. First, <span class="math inline">\(\mathbb{1}_{A}(x)\)</span> is what’s known in mathematics as an “indicator function” that takes one of two possible values:</p>
<p><span class="math display">\[
\mathbb{1}_{A}(x) = \left\{
\begin{array}{ll}
1 &amp; \text{if } x \text{ is in } A \\
0 &amp; \text{if } \text{otherwise} \end{array}
\right.
\]</span></p>
<p>In a statistical modeling context this is also known as a “dummy variable”. In our case, let’s consider the first such indicator variable:</p>
<p><span class="math display">\[
\mathbb{1}_{\mbox{Amer}}(x) = \left\{
\begin{array}{ll}
1 &amp; \text{if } \text{country } x \text{ is in the Americas} \\
0 &amp; \text{otherwise}\end{array}
\right.
\]</span></p>
<p>Now let’s interpret the terms in the estimate column of the regression table. First <span class="math inline">\(b_0 =\)</span> <code>intercept = 54.8</code> corresponds to the mean life expectancy for countries in Africa, since for country <span class="math inline">\(x\)</span> in Africa we have the following equation:</p>
<p><span class="math display">\[
\begin{align}
\widehat{\text{life exp}} &amp;= b_0 + b_{\text{Amer}}\cdot\mathbb{1}_{\mbox{Amer}}(x) + b_{\text{Asia}}\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ b_{\text{Euro}}\cdot\mathbb{1}_{\mbox{Euro}}(x) + b_{\text{Ocean}}\cdot\mathbb{1}_{\mbox{Ocean}}(x)\\
&amp;= 54.8 + 18.8\cdot\mathbb{1}_{\mbox{Amer}}(x) + 15.9\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ 22.8\cdot\mathbb{1}_{\mbox{Euro}}(x) + 25.9\cdot\mathbb{1}_{\mbox{Ocean}}(x)\\
&amp;= 54.8 + 18.8\cdot 0 + 15.9\cdot 0 + 22.8\cdot 0 + 25.9\cdot 0\\
&amp;= 54.8
\end{align}
\]</span></p>
<p>i.e. All four of the indicator variables are equal to 0. Recall we stated earlier that we would treat Africa as the baseline for comparison group. Furthermore, this value corresponds to the group mean life expectancy for all African countries in Table <a href="#tab:continent-mean-life-expectancies">1.7</a>.</p>
<p>Next, <span class="math inline">\(b_{\text{Amer}}\)</span> = <code>continentAmericas = 18.8</code> is the difference in mean life expectancies of countries in the Americas relative to Africa, or in other words, on average countries in the Americas had life expectancy 18.8 years greater. The fitted value yielded by this equation is:</p>
<p><span class="math display">\[
\begin{align}
\widehat{\text{life exp}} &amp;= b_0 + b_{\text{Amer}}\cdot\mathbb{1}_{\mbox{Amer}}(x) + b_{\text{Asia}}\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ b_{\text{Euro}}\cdot\mathbb{1}_{\mbox{Euro}}(x) + b_{\text{Ocean}}\cdot\mathbb{1}_{\mbox{Ocean}}(x)\\
&amp;= 54.8 + 18.8\cdot\mathbb{1}_{\mbox{Amer}}(x) + 15.9\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ 22.8\cdot\mathbb{1}_{\mbox{Euro}}(x) + 25.9\cdot\mathbb{1}_{\mbox{Ocean}}(x)\\
&amp;= 54.8 + 18.8\cdot 1 + 15.9\cdot 0 + 22.8\cdot 0 + 25.9\cdot 0\\
&amp;= 54.8 + 18.8\\
&amp;= 72.9
\end{align}
\]</span></p>
<p>i.e. in this case, only the indicator function <span class="math inline">\(\mathbb{1}_{\mbox{Amer}}(x)\)</span> is equal to 1, but all others are 0. Recall that 72.9 corresponds to the group mean life expectancy for all countries in the Americas in Table <a href="#tab:continent-mean-life-expectancies">1.7</a>.</p>
<p>Similarly, <span class="math inline">\(b_{\text{Asia}}\)</span> = <code>continentAsia = 15.9</code> is the difference in mean life expectancies of Asian countries relative to Africa countries, or in other words, on average countries in the Asia had life expectancy 18.8 years greater than Africa. The fitted value yielded by this equation is:</p>
<p><span class="math display">\[
\begin{align}
\widehat{\text{life exp}} &amp;= b_0 + b_{\text{Amer}}\cdot\mathbb{1}_{\mbox{Amer}}(x) + b_{\text{Asia}}\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ b_{\text{Euro}}\cdot\mathbb{1}_{\mbox{Euro}}(x) + b_{\text{Ocean}}\cdot\mathbb{1}_{\mbox{Ocean}}(x)\\
&amp;= 54.8 + 18.8\cdot\mathbb{1}_{\mbox{Amer}}(x) + 15.9\cdot\mathbb{1}_{\mbox{Asia}}(x)
+ 22.8\cdot\mathbb{1}_{\mbox{Euro}}(x) + 25.9\cdot\mathbb{1}_{\mbox{Ocean}}(x)\\
&amp;= 54.8 + 18.8\cdot 0 + 15.9\cdot 1 + 22.8\cdot 0 + 25.9\cdot 0\\
&amp;= 54.8 + 15.9\\
&amp;= 70.7
\end{align}
\]</span></p>
<p>i.e. in this case, only the indicator function <span class="math inline">\(\mathbb{1}_{\mbox{Asia}}(x)\)</span> is equal to 1, but all others are 0. Recall that 70.7 corresponds to the group mean life expectancy for all countries in Asia in Table <a href="#tab:continent-mean-life-expectancies">1.7</a>. The same logic applies to <span class="math inline">\(b_{\text{Euro}} = 22.8\)</span> and <span class="math inline">\(b_{\text{Ocean}} = 25.9\)</span>; they correspond to the “offset” in mean life expectancy for countries in Europe and Oceania, relative to the mean life expectancy of the baseline group for comparison of African countries.</p>
<p>Let’s generalize this idea a bit. If we fit a linear regression model using a categorical explanatory variable <span class="math inline">\(x\)</span> that has <span class="math inline">\(k\)</span> levels, a regression model will return an intercept and <span class="math inline">\(k - 1\)</span> “slope” coefficients. When <span class="math inline">\(x\)</span> is a numerical explanatory variable the interpretation is of a “slope” coefficient, but when <span class="math inline">\(x\)</span> is categorical the meaning is a little trickier. They are <em>offsets</em> relative to the baseline.</p>
<p>In our case, since there are <span class="math inline">\(k = 5\)</span> continents, the regression model returns an intercept corresponding to the baseline for comparison Africa and <span class="math inline">\(k - 1 = 4\)</span> slope coefficients corresponding to the Americas, Asia, Europe, and Oceania. Africa was chosen as the baseline by R for no other reason than it is first alphabetically of the 5 continents. You can manually specify which continent to use as baseline instead of the default choice of whichever comes first alphabetically, but we leave that to a more advanced course. (The <code>forcats</code> package is particularly nice for doing this and we encourage you to explore using it.)</p>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC6.5)</strong> Fit a new linear regression using <code>lm(gdpPercap ~ continent, data = gapminder2007)</code> where <code>gdpPercap</code> is the new outcome variable <span class="math inline">\(y\)</span>. Get information about the “best-fitting” line from the regression table by applying the <code>get_regression_table()</code> function. How do the regression results match up with the results from your exploratory data analysis above?</p>
<div class="learncheck">

</div>
</div>
<div id="model2points" class="section level3">
<h3><span class="header-section-number">1.2.3</span> 观察值,拟合值,残差</h3>
<p>Recall in Subsection <a href="#model1points">1.1.3</a> when we had a numerical explanatory variable <span class="math inline">\(x\)</span>, we defined:</p>
<ol style="list-style-type: decimal">
<li>Observed values <span class="math inline">\(y\)</span>, or the observed value of the outcome variable</li>
<li>Fitted values <span class="math inline">\(\widehat{y}\)</span>, or the value on the regression line for a given <span class="math inline">\(x\)</span> value</li>
<li>Residuals <span class="math inline">\(y - \widehat{y}\)</span>, or the error between the observed value and the fitted value</li>
</ol>
<p>What do fitted values <span class="math inline">\(\widehat{y}\)</span> and residuals <span class="math inline">\(y - \widehat{y}\)</span> correspond to when the explanatory variable <span class="math inline">\(x\)</span> is categorical? Let’s investigate these values for the first 10 countries in the <code>gapminder2007</code> dataset:</p>
<table>
<caption><span id="tab:unnamed-chunk-39">Table 1.9: </span>First 10 out of 142 countries</caption>
<thead>
<tr class="header">
<th align="left">country</th>
<th align="left">continent</th>
<th align="right">lifeExp</th>
<th align="right">gdpPercap</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Afghanistan</td>
<td align="left">Asia</td>
<td align="right">43.8</td>
<td align="right">975</td>
</tr>
<tr class="even">
<td align="left">Albania</td>
<td align="left">Europe</td>
<td align="right">76.4</td>
<td align="right">5937</td>
</tr>
<tr class="odd">
<td align="left">Algeria</td>
<td align="left">Africa</td>
<td align="right">72.3</td>
<td align="right">6223</td>
</tr>
<tr class="even">
<td align="left">Angola</td>
<td align="left">Africa</td>
<td align="right">42.7</td>
<td align="right">4797</td>
</tr>
<tr class="odd">
<td align="left">Argentina</td>
<td align="left">Americas</td>
<td align="right">75.3</td>
<td align="right">12779</td>
</tr>
<tr class="even">
<td align="left">Australia</td>
<td align="left">Oceania</td>
<td align="right">81.2</td>
<td align="right">34435</td>
</tr>
<tr class="odd">
<td align="left">Austria</td>
<td align="left">Europe</td>
<td align="right">79.8</td>
<td align="right">36126</td>
</tr>
<tr class="even">
<td align="left">Bahrain</td>
<td align="left">Asia</td>
<td align="right">75.6</td>
<td align="right">29796</td>
</tr>
<tr class="odd">
<td align="left">Bangladesh</td>
<td align="left">Asia</td>
<td align="right">64.1</td>
<td align="right">1391</td>
</tr>
<tr class="even">
<td align="left">Belgium</td>
<td align="left">Europe</td>
<td align="right">79.4</td>
<td align="right">33693</td>
</tr>
</tbody>
</table>
<p>Recall the <code>get_regression_points()</code> function we used in Subsection <a href="#model1points">1.1.3</a> to return</p>
<ul>
<li>the observed value of the outcome variable,</li>
<li>all explanatory variables,</li>
<li>fitted values, and</li>
<li>residuals for all points in the regression. Recall that each “point”. In this case, each row corresponds to one of 142 countries in the <code>gapminder2007</code> dataset. They are also the 142 observations used to construct the boxplots in Figure <a href="#fig:catxplot1">1.15</a>.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r">regression_points &lt;-<span class="st"> </span><span class="kw">get_regression_points</span>(lifeExp_model)
regression_points</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-41">Table 1.10: </span>Regression points (First 10 out of 142 countries)</caption>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="right">lifeExp</th>
<th align="left">continent</th>
<th align="right">lifeExp_hat</th>
<th align="right">residual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">43.8</td>
<td align="left">Asia</td>
<td align="right">70.7</td>
<td align="right">-26.900</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">76.4</td>
<td align="left">Europe</td>
<td align="right">77.6</td>
<td align="right">-1.226</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">72.3</td>
<td align="left">Africa</td>
<td align="right">54.8</td>
<td align="right">17.495</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">42.7</td>
<td align="left">Africa</td>
<td align="right">54.8</td>
<td align="right">-12.075</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">75.3</td>
<td align="left">Americas</td>
<td align="right">73.6</td>
<td align="right">1.712</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">81.2</td>
<td align="left">Oceania</td>
<td align="right">80.7</td>
<td align="right">0.515</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="right">79.8</td>
<td align="left">Europe</td>
<td align="right">77.6</td>
<td align="right">2.180</td>
</tr>
<tr class="even">
<td align="right">8</td>
<td align="right">75.6</td>
<td align="left">Asia</td>
<td align="right">70.7</td>
<td align="right">4.907</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="right">64.1</td>
<td align="left">Asia</td>
<td align="right">70.7</td>
<td align="right">-6.666</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="right">79.4</td>
<td align="left">Europe</td>
<td align="right">77.6</td>
<td align="right">1.792</td>
</tr>
</tbody>
</table>
<p>Notice</p>
<ul>
<li>The fitted values <code>lifeExp_hat</code> <span class="math inline">\(\widehat{\text{lifeexp}}\)</span>. Countries in Africa have the
same fitted value of 54.8, which is the mean life expectancy of Africa. Countries in Asia have the same fitted value of 70.7, which is the mean life
expectancy of Asia. This similarly holds for countries in the Americas, Europe,
and Oceania.</li>
<li>The <code>residual</code> column is simply <span class="math inline">\(y - \widehat{y}\)</span> = <code>lifeexp - lifeexp_hat</code>.
These values can be interpreted as that particular country’s deviation from the
mean life expectancy of the respective continent’s mean. For example, the first
row of this dataset corresponds to Afghanistan, and the residual of
<span class="math inline">\(-26.9 = 43.8 - 70.7\)</span> is Afghanistan’s mean life expectancy minus the mean life
expectancy of all Asian countries.</li>
</ul>
</div>
<div id="model2residuals" class="section level3">
<h3><span class="header-section-number">1.2.4</span> 残差分析</h3>
<p>Recall our discussion on residuals from Section <a href="#model1residuals">1.1.4</a> where our goal was to investigate whether or not there was a <em>systematic pattern</em> to the residuals. Ideally since residuals can be thought of as error, there should be no such pattern. While there are many ways to do such residual analysis, we focused on two approaches based on visualizations.</p>
<ol style="list-style-type: decimal">
<li>A plot with residuals on the vertical axis and the predictor (in this case continent) on the horizontal axis</li>
<li>A histogram of all residuals</li>
</ol>
<p>First, let’s plot the residuals versus continent in Figure <a href="#fig:catxplot7">1.16</a>, but also let’s plot all 142 points with a little horizontal random jitter by setting the <code>width = 0.1</code> parameter in <code>geom_jitter()</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(regression_points, <span class="kw">aes</span>(<span class="dt">x =</span> continent, <span class="dt">y =</span> residual)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="fl">0.1</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Continent&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Residual&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>)</code></pre>
<div class="figure"><span id="fig:catxplot7"></span>
<img src="06-regression_files/figure-html/catxplot7-1.png" alt="Plot of residuals over continent" width="\textwidth" />
<p class="caption">
Figure 1.16: Plot of residuals over continent
</p>
</div>
<p>We observe</p>
<ol style="list-style-type: decimal">
<li>There seems to be a rough balance of both positive and negative residuals for all 5 continents.</li>
<li>However, there is one clear outlier in Asia. It has the smallest residual,
hence also has the smallest life expectancy in Asia.</li>
</ol>
<p>Let’s investigate the 5 countries in Asia with the shortest life expectancy:</p>
<pre class="sourceCode r"><code class="sourceCode r">gapminder2007 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(continent <span class="op">==</span><span class="st"> &quot;Asia&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(lifeExp)</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-43">Table 1.11: </span>Countries in Asia with shortest life expectancy</caption>
<thead>
<tr class="header">
<th align="left">country</th>
<th align="left">continent</th>
<th align="right">lifeExp</th>
<th align="right">gdpPercap</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Afghanistan</td>
<td align="left">Asia</td>
<td align="right">43.8</td>
<td align="right">975</td>
</tr>
<tr class="even">
<td align="left">Iraq</td>
<td align="left">Asia</td>
<td align="right">59.5</td>
<td align="right">4471</td>
</tr>
<tr class="odd">
<td align="left">Cambodia</td>
<td align="left">Asia</td>
<td align="right">59.7</td>
<td align="right">1714</td>
</tr>
<tr class="even">
<td align="left">Myanmar</td>
<td align="left">Asia</td>
<td align="right">62.1</td>
<td align="right">944</td>
</tr>
<tr class="odd">
<td align="left">Yemen, Rep.</td>
<td align="left">Asia</td>
<td align="right">62.7</td>
<td align="right">2281</td>
</tr>
</tbody>
</table>
<p>This was the earlier identified residual for Afghanistan of -26.9. Unfortunately
given recent geopolitical turmoil, individuals who live in Afghanistan and, in particular in 2007, have a
drastically lower life expectancy.</p>
<p>Second, let’s look at a histogram of all 142 values of
residuals in Figure <a href="#fig:catxplot8">1.17</a>. In this case, the residuals form a
rather nice bell-shape, although there are a couple of very low and very high
values at the tails. As we said previously, searching for patterns in residuals
can be somewhat subjective, but ideally we hope there are no “drastic” patterns.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(regression_points, <span class="kw">aes</span>(<span class="dt">x =</span> residual)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">5</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Residual&quot;</span>)</code></pre>
<div class="figure"><span id="fig:catxplot8"></span>
<img src="06-regression_files/figure-html/catxplot8-1.png" alt="Histogram of residuals" width="\textwidth" />
<p class="caption">
Figure 1.17: Histogram of residuals
</p>
</div>
<div class="learncheck">
<p>
<strong><em>Learning check</em></strong>
</p>
</div>
<p><strong>(LC6.6)</strong> Continuing with our regression using <code>gdpPercap</code> as the outcome variable and <code>continent</code> as the explanatory variable, use the <code>get_regression_points()</code> function to get the observed values, fitted values, and residuals for all 142 countries in 2007 and perform a residual analysis to look for any systematic patterns in the residuals. Is there a pattern? Please keep in mind that these types of questions are somewhat subjective and different people will most likely have different answers. The focus should be on being able to justify the conclusions made.</p>
<div class="learncheck">

</div>
</div>
</div>
<div id="section-1.3" class="section level2">
<h2><span class="header-section-number">1.3</span> 其他内容</h2>
<div id="correlationcoefficient" class="section level3">
<h3><span class="header-section-number">1.3.1</span> 相关系数</h3>
<p>Let’s re-plot Figure <a href="#fig:correlation1">1.1</a>, but now consider a broader range of correlation coefficient values in Figure <a href="#fig:correlation2">1.18</a>.</p>
<div class="figure"><span id="fig:correlation2"></span>
<img src="06-regression_files/figure-html/correlation2-1.png" alt="Different Correlation Coefficients" width="\textwidth" />
<p class="caption">
Figure 1.18: Different Correlation Coefficients
</p>
</div>
<p>As we suggested in Subsection <a href="#model1EDA">1.1.1</a>, interpreting coefficients that are not close to the extreme values of -1 and 1 can be subjective. To develop your sense of correlation coefficients, we suggest you play the following 80’s-style video game called “Guess the correlation”! Click on the image below to do so:</p>
<center>
<a target="_blank" href="http://guessthecorrelation.com/"><img src="images/guess_the_correlation.png" title="Guess the correlation" width="600"/></a>
</center>
</div>
<div id="correlation-is-not-causation" class="section level3">
<h3><span class="header-section-number">1.3.2</span> 相关关系不一定是因果关系</h3>
<p>You’ll note throughout this chapter we’ve been very cautious in making statements of the “associated effect” of explanatory variables on the outcome variables, for example our statement from Subsection <a href="#model1table">1.1.2</a> that “for every increase of 1 unit in <code>bty_avg</code>, there is an <em>associated</em> increase of, <em>on average</em>, 18.802 units of <code>score</code>.” We stay this because we are careful not to make <em>causal</em> statements. So while beauty score <code>bty_avg</code> is positively correlated with teaching <code>score</code>, does it directly cause effects on teaching score.</p>
<p>For example, let’s say an instructor has their <code>bty_avg</code> reevaluated, but only after taking steps to try to boost their beauty score. Does this mean that they will suddenly be a better instructor? Or will they suddenly get higher teaching scores? Maybe?</p>
<p>Here is another example, a not-so-great medical doctor goes through their medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares “Sleeping with shoes on cause headaches!”</p>
<div class="figure" style="text-align: center"><span id="fig:moderndive-figure-causal-graph-2"></span>
<img src="images/flowcharts/flowchart.010-cropped.png" alt="Does sleeping with shoes on cause headaches?" width="\textwidth" />
<p class="caption">
Figure 1.19: Does sleeping with shoes on cause headaches?
</p>
</div>
<p>However as some of you might have guessed, if someone is sleeping with their shoes on its probably because they are intoxicated. Furthermore, drinking more tends to cause more hangovers, and hence more headaches.</p>
<p>In this instance, alcohol is what’s known as a <em>confounding/lurking</em> variable. It “lurks” behind the scenes, confounding or making less apparent, the causal effect (if any) of “sleeping with shoes on” with waking up with a headache. We can summarize this notion in Figure <a href="#fig:moderndive-figure-causal-graph">1.20</a> with a <em>causal graph</em> where:</p>
<ul>
<li>Y: Is an <em>outcome</em> variable, here “waking up with a headache.”</li>
<li>X: Is a <em>treatment</em> variable whose causal effect we are interested in, here “sleeping with shoes on.”</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:moderndive-figure-causal-graph"></span>
<img src="images/flowcharts/flowchart.009-cropped.png" alt="Causal graph." width="\textwidth" />
<p class="caption">
Figure 1.20: Causal graph.
</p>
</div>
<p>So for example, many such studies use regression modeling where the outcome variable is set to Y and the explanatory/predictor variable is X, much as you’ve started learning how to do in this chapter. However, Figure <a href="#fig:moderndive-figure-causal-graph">1.20</a> also includes a third variable with arrows pointing at both X and Y.</p>
<ul>
<li>Z: Is a <em>confounding</em> variable that effects both X &amp; Y, thus “confounding” their relationship.</li>
</ul>
<p>So as we said, alcohol will both cause people to be more likely to sleep with their shoes on as well as more likely to wake up with a headache. Thus when evaluating what causes one to wake up with a headache, its hard to tease out the effect of sleeping with shoes on versus just the alcohol. Thus our model needs to also use Z as an explanatory/predictor variable as well, in other words our doctor needs to take into account who had been drinking the night before. We’ll start covering multiple regression models that allows us to incorporate more than one variable in the next chapter.</p>
<p>Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of potential confounding variables. Both these approaches attempt either to remove all confounding variables or take them into account as best they can, and only focus on the behavior of an outcome variable in the presence of the levels of the other variable(s). Be careful as you read studies to make sure that the writers aren’t falling into this fallacy of correlation implying causation. If you spot one, you may want to send them a link to <a href="http://www.tylervigen.com/spurious-correlations">Spurious Correlations</a>.</p>
</div>
<div id="leastsquares" class="section level3">
<h3><span class="header-section-number">1.3.3</span> 最佳拟合线</h3>
<p>Regression lines are also known as “best fitting lines”. But what do we mean by best? Let’s unpack the criteria
that is used by regression to determine best. Recall the plot in Figure <a href="#fig:numxplot5">1.8</a> where for a instructor
with a beauty average score of <span class="math inline">\(x=7.333\)</span></p>
<ul>
<li>The observed value <span class="math inline">\(y=4.9\)</span> was marked with a red circle</li>
<li>The fitted value <span class="math inline">\(\widehat{y} = 4.369\)</span> on the regression line was marked with a red square</li>
<li>The residual <span class="math inline">\(y-\widehat{y} = 4.9-4.369 = 0.531\)</span> was the length of the blue arrow.</li>
</ul>
<p>Let’s do this for another arbitrarily chosen instructor whose beauty score was
<span class="math inline">\(x=2.333\)</span>. The residual in this case is <span class="math inline">\(2.7 - 4.036 = -1.336\)</span>.</p>
<p><img src="06-regression_files/figure-html/unnamed-chunk-46-1.png" width="\textwidth" /></p>
<p>Another arbitrarily chosen instructor whose beauty score was
<span class="math inline">\(x=3.667\)</span> results in the residual in this case being <span class="math inline">\(4.4 - 4.125 = 0.2753\)</span>.</p>
<p><img src="06-regression_files/figure-html/unnamed-chunk-47-1.png" width="\textwidth" /></p>
<p>Let’s do this one more time for another arbitrarily chosen instructor. This instructor had a beauty score of
<span class="math inline">\(x = 6\)</span>. The residual in this case is <span class="math inline">\(3.8 - 4.28 = -0.4802\)</span>.</p>
<p><img src="06-regression_files/figure-html/here-1.png" width="\textwidth" /></p>
<p>Now let’s say we repeated this process for all 463 instructors in our
dataset. Regression <em>minimizes the sum of all 463 arrow lengths
squared.</em> In other words, it minimizes the sum of the squared residuals:</p>
<p><span class="math display">\[
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
\]</span></p>
<p>We square the arrow lengths so that positive and negative deviations of the same amount are treated equally. That’s why alternative names for the simple linear regression line are the <strong>least-squares line</strong> and the <strong>best fitting line</strong>. It can be proven via calculus and linear algebra that this line uniquely minimizes the sum of the squared arrow lengths.</p>
<p>For the regression line in the plot, the sum of the squared residuals is 131.879. This is the lowest possible value of the sum of the squared residuals of all possible lines we could draw on this scatterplot? How do we know this? We can mathematically prove this fact, but this requires some calculus and linear algebra, so let’s leave this proof for another course!</p>
</div>
<div id="underthehood" class="section level3">
<h3><span class="header-section-number">1.3.4</span> <code>get_regression_x()</code> 功能</h3>
<p>What is going on behind the scenes with the <code>get_regression_table()</code> <code>get_regression_points()</code> from the <code>moderndive</code> package? Recall we introduced</p>
<ol style="list-style-type: decimal">
<li>In Subsection <a href="#model1table">1.1.2</a>, the <code>get_regression_table()</code> function that returned a regression table.</li>
<li>In Subsection <a href="#model1points">1.1.3</a>, the <code>get_regression_points()</code> function that returned information on all <span class="math inline">\(n\)</span> points/observations involved in a regression?</li>
</ol>
<p>and that these were examples of <em>wrapper functions</em> that takes other pre-existing functions and “wraps” them in a single function. This way all the user needs to worry about is the input and the output format, and ignore what’s “under the hood.” In this subsection we “lift the hood” and see how the engine of these wrapper functions work.</p>
<p>First, the <code>get_regression_table()</code> wrapper function leverages the</p>
<ul>
<li>the <code>tidy()</code> function in the <a href="https://broom.tidyverse.org/"><code>broom</code> package</a> and</li>
<li>the <code>clean_names()</code> function in the <a href="https://github.com/sfirke/janitor"><code>janitor</code> package</a></li>
</ul>
<p>to generate tidy data frames with information about a regression model. Here is what the regression table from Subsection <a href="#model1table">1.1.2</a> looks like:</p>
<pre class="sourceCode r"><code class="sourceCode r">score_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score <span class="op">~</span><span class="st"> </span>bty_avg, <span class="dt">data =</span> evals_ch6)
<span class="kw">get_regression_table</span>(score_model)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std_error</th>
<th align="right">statistic</th>
<th align="right">p_value</th>
<th align="right">lower_ci</th>
<th align="right">upper_ci</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">intercept</td>
<td align="right">3.880</td>
<td align="right">0.076</td>
<td align="right">50.96</td>
<td align="right">0</td>
<td align="right">3.731</td>
<td align="right">4.030</td>
</tr>
<tr class="even">
<td align="left">bty_avg</td>
<td align="right">0.067</td>
<td align="right">0.016</td>
<td align="right">4.09</td>
<td align="right">0</td>
<td align="right">0.035</td>
<td align="right">0.099</td>
</tr>
</tbody>
</table>
<p>The <code>get_regression_table()</code> function takes the above two functions that already existed in other R packages, uses them, and hides the details as seen below. This was on the editorial decision on our part as we felt the following code was unfortunately out of the reach for some new coders, so the following wrapper function was written so that users need only focus on the output.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)
<span class="kw">library</span>(janitor)
score_model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">tidy</span>(<span class="dt">conf.int =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate_if</span>(is.numeric, round, <span class="dt">digits =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">clean_names</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">rename</span>(<span class="dt">lower_ci =</span> conf_low,
         <span class="dt">upper_ci =</span> conf_high)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">term</th>
<th align="right">estimate</th>
<th align="right">std_error</th>
<th align="right">statistic</th>
<th align="right">p_value</th>
<th align="right">lower_ci</th>
<th align="right">upper_ci</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">3.880</td>
<td align="right">0.076</td>
<td align="right">50.96</td>
<td align="right">0</td>
<td align="right">3.731</td>
<td align="right">4.030</td>
</tr>
<tr class="even">
<td align="left">bty_avg</td>
<td align="right">0.067</td>
<td align="right">0.016</td>
<td align="right">4.09</td>
<td align="right">0</td>
<td align="right">0.035</td>
<td align="right">0.099</td>
</tr>
</tbody>
</table>
<p>Note that the <code>mutate_if()</code> function is from the <code>dplyr</code> package and applies the <code>round()</code> function with 3 significant digits precision only to those variables that are numerical.</p>
<p>Similarly, the second <code>get_regression_points()</code> function is another wrapper function, but this time returning information about the points in a regression rather than the regression table. It uses the <code>augment()</code> function in the <a href="https://broom.tidyverse.org/"><code>broom</code> package</a> instead of <code>tidy()</code> as with <code>get_regression_points()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)
<span class="kw">library</span>(janitor)
score_model <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">augment</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate_if</span>(is.numeric, round, <span class="dt">digits =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">clean_names</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span><span class="kw">c</span>(<span class="st">&quot;se_fit&quot;</span>, <span class="st">&quot;hat&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;cooksd&quot;</span>, <span class="st">&quot;std_resid&quot;</span>))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">score</th>
<th align="right">bty_avg</th>
<th align="right">fitted</th>
<th align="right">resid</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4.7</td>
<td align="right">5.00</td>
<td align="right">4.21</td>
<td align="right">0.486</td>
</tr>
<tr class="even">
<td align="right">4.1</td>
<td align="right">5.00</td>
<td align="right">4.21</td>
<td align="right">-0.114</td>
</tr>
<tr class="odd">
<td align="right">3.9</td>
<td align="right">5.00</td>
<td align="right">4.21</td>
<td align="right">-0.314</td>
</tr>
<tr class="even">
<td align="right">4.8</td>
<td align="right">5.00</td>
<td align="right">4.21</td>
<td align="right">0.586</td>
</tr>
<tr class="odd">
<td align="right">4.6</td>
<td align="right">3.00</td>
<td align="right">4.08</td>
<td align="right">0.520</td>
</tr>
<tr class="even">
<td align="right">4.3</td>
<td align="right">3.00</td>
<td align="right">4.08</td>
<td align="right">0.220</td>
</tr>
<tr class="odd">
<td align="right">2.8</td>
<td align="right">3.00</td>
<td align="right">4.08</td>
<td align="right">-1.280</td>
</tr>
<tr class="even">
<td align="right">4.1</td>
<td align="right">3.33</td>
<td align="right">4.10</td>
<td align="right">-0.002</td>
</tr>
<tr class="odd">
<td align="right">3.4</td>
<td align="right">3.33</td>
<td align="right">4.10</td>
<td align="right">-0.702</td>
</tr>
<tr class="even">
<td align="right">4.5</td>
<td align="right">3.17</td>
<td align="right">4.09</td>
<td align="right">0.409</td>
</tr>
</tbody>
</table>
<p>In this case, it outputs only variables of interest to us as new regression modelers: the outcome variable <span class="math inline">\(y\)</span> (<code>score</code>), all explanatory/predictor variables (<code>bty_avg</code>), all resulting <code>fitted</code> values <span class="math inline">\(\hat{y}\)</span> used by applying the equation of the regression line to <code>bty_avg</code>, and the <code>resid</code>ual <span class="math inline">\(y - \hat{y}\)</span>.</p>
<p>If you’re even more curious, take a look at the source code for these functions on <a href="https://github.com/moderndive/moderndive/blob/master/R/regression_functions.R">GitHub</a>.</p>
</div>
</div>
<div id="section-1.4" class="section level2">
<h2><span class="header-section-number">1.4</span> 结论</h2>
<p>在本章节中，你学习到了当只有一个解释变量的’基本回归分析’。在’多元回归分析’中，我们将学习多个解释变量。特别是，我们将学习为什么要做残差分析。其实我们在验证一些非常重要的假设。只有当这些假设成立时，在回归结果表中的std_error (标准误差), p_value, lower_ci and upper_ci (
置信区间) 才能构成有效的解释。如果目前您好不了解，无需担心。我们将在下一节中进行讲解。</p>
<div id="r" class="section level3">
<h3><span class="header-section-number">1.4.1</span> R代码</h3>
<p>本章节《基本回归分析》的所有代码:
<a href="https://moderndive.com/scripts/06-regression.R">here</a>.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/moderndive/moderndive_book/edit/master/%s",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
